{"cells":[{"metadata":{"_cell_guid":"ea25cdf7-bdbc-3cf1-0737-bc51675e3374","_uuid":"fed5696c67bf55a553d6d04313a77e8c617cad99"},"cell_type":"markdown","source":"# ♪タイタニック","execution_count":null},{"metadata":{"_cell_guid":"5767a33c-8f18-4034-e52d-bf7a8f7d8ab8","_uuid":"847a9b3972a6be2d2f3346ff01fea976d92ecdb6","trusted":true},"cell_type":"code","source":"# データ分析・整理\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# 可視化\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# 機械学習\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e7319668-86fe-8adc-438d-0eef3fd0a982","_uuid":"13f38775c12ad6f914254a08f0d1ef948a2bd453","trusted":true},"cell_type":"code","source":"# データ取得\ntrain_df = pd.read_csv('../input/titanic/train.csv')\ntest_df = pd.read_csv('../input/titanic/test.csv')\ncombine = [train_df, test_df]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ce473d29-8d19-76b8-24a4-48c217286e42","_uuid":"ef106f38a00e162a80c523778af6dcc778ccc1c2","trusted":true},"cell_type":"code","source":"print(train_df.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f","_uuid":"e068cd3a0465b65a0930a100cb348b9146d5fd2f","trusted":true},"cell_type":"code","source":"# データのプレチェック\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f6e761c2-e2ff-d300-164c-af257083bb46","_uuid":"3488e80f309d29f5b68bbcfaba8d78da84f4fb7d","trusted":true},"cell_type":"code","source":"train_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9b805f69-665a-2b2e-f31d-50d87d52865d","_uuid":"817e1cf0ca1cb96c7a28bb81192d92261a8bf427","trusted":true},"cell_type":"code","source":"# データ情報チェック\ntrain_df.info()\nprint('_'*40)\ntest_df.info()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"859102e1-10df-d451-2649-2d4571e5f082","_uuid":"2b7c205bf25979e3242762bfebb0e3eb2fd63010"},"cell_type":"markdown","source":"**サンプル間の特徴量の数値分布はどのようになっているか？\n\nこれにより、特に初期の段階では、訓練データが実際の問題領域の代表的なデータセットであるかどうかを判断するのに役立ちます。\n\n- タイタニック号に乗っていた実際の乗客数（2,224人）の40%にあたる891人のサンプルを合計しています。\n- 生存しているかどうかは、0または1の値を持つカテゴリカルな特徴量です。\n- 約38％のサンプルが生存しており、実際の生存率32％を代表しています。\n- ほとんどの乗客（75％以上）は親子連れではありませんでした。\n- 乗客の30％近くが兄弟や配偶者を連れていました。\n- 運賃には大きなばらつきがあり、512ドルという高額な料金を支払っている乗客はほとんどいませんでした（1％未満）。\n- 65歳から80歳までの高齢者の乗客はほとんどいませんでした（1％未満）。","execution_count":null},{"metadata":{"_cell_guid":"58e387fe-86e4-e068-8307-70e37fe3f37b","_uuid":"380251a1c1e0b89147d321968dc739b6cc0eecf2","trusted":true},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8066b378-1964-92e8-1352-dcac934c6af3","_uuid":"daa8663f577f9c1a478496cf14fe363570457191","trusted":true},"cell_type":"code","source":"train_df.describe(include=['O'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2cb22b88-937d-6f14-8b06-ea3361357889","_uuid":"c1d35ebd89a0cf7d7b409470bbb9ecaffd2a9680"},"cell_type":"markdown","source":"### データ分析に基づく仮定\n\nこれまでのデータ分析に基づき、以下のような前提条件を設定しています。これらの仮定を検証した上で、適切な対応を行うことがあります。\n\n**相関性\n\n各機能が生存率とどの程度相関しているかを知りたいと思います。プロジェクトの初期段階でこれを行い、プロジェクトの後半でモデル化された相関との迅速な相関を一致させたいと考えています。\n\n**完了。\n\n1. 生存率と確実に相関しているので、年齢の特徴を完成させたいと思うかもしれません。\n2. 2. 生存または別の重要な特徴と相関があるかもしれないので、エンバークされた特徴を完成させたいかもしれません。\n\n**修正中。\n\n1. チケット機能は重複率が高く(22%)、チケットと生存率の間に相関がない可能性があるため、分析から除外されるかもしれません。\n2. 2. キャビン特徴は、訓練データとテストデータの両方において、非常に不完全であったり、ヌル値を多く含んでいたりするため、削除される可能性がある。\n3. 3. PassengerIdは生存率に寄与しないため，訓練データセットから削除される可能性がある．\n4. 4. 名前特徴は比較的標準的ではないため、生存に直接寄与しない可能性があるため、削除されるかもしれません。\n\n**作成。\n\n1. ParchとSibSpに基づいたFamilyという新しい機能を作成して、搭乗している家族の総数を取得したいと思うかもしれません。\n2. 2. 我々は、新しい機能としてタイトルを抽出するために名前機能をエンジニアにしたい場合があります。\n3. 3. 年齢バンドのための新しい機能を作成することもできます。これにより、連続した数値特徴を順序的なカテゴリ特徴に変えることができます。\n4. 4. 分析に役立つのであれば、運賃範囲特徴を作成することもできます。\n\n**分類。\n\n前述の問題記述に基づいて、仮定を追加することもできます。\n\n1. 女性 (Sex=female) の方が生存している可能性が高かった。\n2. 2. 子供 (Age<?) は生存している可能性が高かった。\n3. 上流階級の乗客(Pclass=1)の方が生存している可能性が高かった。\n","execution_count":null},{"metadata":{"_cell_guid":"6db63a30-1d86-266e-2799-dded03c45816","_uuid":"946ee6ca01a3e4eecfa373ca00f88042b683e2ad"},"cell_type":"markdown","source":"## ピボット機能を使って分析する\n\n我々の観察と仮定のいくつかを確認するために、特徴を互いに回転させることで、特徴の相関を素早く分析することができます。この段階でできるのは、空の値を持たない特徴に対してのみです。また、カテゴリ型（Sex）、順序型（Pclass）、または離散型（SibSp, Parch）の特徴に対してのみそうすることも理にかなっています。\n\n- Pclass** Pclass=1とSurvived (分類#3)の間には、有意な相関が見られます (>0.5)。この特徴をモデルに含めることにしました。\n- Sex** 問題定義時にSex=女性の生存率が74%と非常に高いことが確認された(分類#1)．\n- SibSpとParch** これらの特徴は、特定の値では相関がゼロになります。これらの個々の特徴から特徴または特徴のセットを導出するのが最善かもしれません（#1の作成）．","execution_count":null},{"metadata":{"_cell_guid":"0964832a-a4be-2d6f-a89e-63526389cee9","_uuid":"97a845528ce9f76e85055a4bb9e97c27091f6aa1","trusted":true},"cell_type":"code","source":"train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"68908ba6-bfe9-5b31-cfde-6987fc0fbe9a","_uuid":"00a2f2bca094c5984e6a232c730c8b232e7e20bb","trusted":true},"cell_type":"code","source":"train_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"01c06927-c5a6-342a-5aa8-2e486ec3fd7c","_uuid":"a8f7a16c54417dcd86fc48aeef0c4b240d47d71b","trusted":true},"cell_type":"code","source":"train_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e686f98b-a8c9-68f8-36a4-d4598638bbd5","_uuid":"5d953a6779b00b7f3794757dec8744a03162c8fd","trusted":true},"cell_type":"code","source":"train_df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0d43550e-9eff-3859-3568-8856570eff76","_uuid":"5c6204d01f5a9040cf0bb7c678686ae48daa201f"},"cell_type":"markdown","source":"## ♪ データを可視化して分析する\n\nこれで、データを分析するための可視化を使って、いくつかの仮定を確認することができるようになりました。\n\n### 数値特徴を相関させる\n\nまず、数値的特徴と我々の目標（生存）との間の相関関係を理解することから始めましょう。\n\nヒストグラム・チャートは、年齢のような連続した数値変数を分析するのに便利で、バンディングや範囲が有用なパターンを特定するのに役立ちます。ヒストグラムは、自動的に定義されたビンまたは等間隔のバンドを使用して、サンプルの分布を示すことができます。これは、特定のバンドに関する質問に答えるのに役立ちます（乳児の方が生存率が高かったのか？\n\nヒストグラムの可視化におけるx軸は、サンプルまたは乗客の数を表していることに注意してください。\n\n**観測。\n\n- 乳児（年齢<=4歳）の生存率が高かった。\n- 最も年配の乗客（年齢 = 80 歳）は生存率が高かった。\n- 15-25歳の乗客の多くは生存していませんでした。\n- 乗客のほとんどは15～35歳の年齢層です。\n\n**決断。\n\nこの単純な分析は、その後のワークフローの段階での決定事項として、我々の仮定を確認するものである。\n\n- モデルトレーニングでは、年齢（我々の仮定の分類#2）を考慮する必要があります。\n- ヌル値の年齢特徴を完成させる（#1を完成させる）。\n- 年齢グループをバンド化する必要があります（#3を作成）。","execution_count":null},{"metadata":{"_cell_guid":"50294eac-263a-af78-cb7e-3778eb9ad41f","_uuid":"d3a1fa63e9dd4f8a810086530a6363c94b36d030","trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(train_df, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"87096158-4017-9213-7225-a19aea67a800","_uuid":"892259f68c2ecf64fd258965cff1ecfe77dd73a9"},"cell_type":"markdown","source":"### 数値特徴と順序特徴を相関させる\n\n1つのプロットを使用して相関関係を識別するために、複数の特徴を組み合わせることができます。これは、数値を持つ数値特徴量とカテゴリ特徴量で行うことができます。\n\n**観察。\n\n- Pclass=3は最も多くの乗客を乗せていましたが、ほとんどの乗客は生き残っていませんでした。Pclass=3の乗客が最も多かったが、ほとんどが生き残っていなかった。\n- Pclass=2とPclass=3の幼児の乗客はほとんどが生存していました。Pclass=2とPclass=3の幼児乗客はほとんどが生存していました。\n- Pclass=1の乗客のほとんどが生存していた。Pclass=1の乗客はほとんどが生存していた。\n- Pクラスは乗客の年齢分布によって異なる。\n\n**決定。\n\n- モデル学習のためにPclassを考慮します。","execution_count":null},{"metadata":{"_cell_guid":"916fdc6b-0190-9267-1ea9-907a3d87330d","_uuid":"4f5bcfa97c8a72f8b413c786954f3a68e135e05a","trusted":true},"cell_type":"code","source":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\ngrid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"36f5a7c0-c55c-f76f-fdf8-945a32a68cb0","_uuid":"892ab7ee88b1b1c5f1ac987884fa31e111bb0507"},"cell_type":"markdown","source":"### カテゴリー特徴の相関関係\n\n今、我々は我々のソリューションの目標とカテゴリの特徴を相関させることができます。\n\n**観察結果。\n\n- 女性の乗客は男性よりもはるかに良い生存率を持っていました。分類(#1)を確認しました。\n- 例外として、Embarked=Cでは男性の方が生存率が高かった。これは、PクラスとEmbarked、そしてPクラスとSurvivedの相関関係である可能性がありますが、必ずしもEmbarkedとSurvivedの直接的な相関関係ではありません。\n- CポートとQポートでは、Pclass=2と比較してPclass=3の方がオスの生存率が高かった。エンバーク（#2）。\n- 乗船した港は、Pclass=3と男性乗客の間で生存率に差がある。相関させる（#1）。\n\n**決定。\n\n- モデル学習にSex特徴を追加。\n- 完了し、モデル学習に乗船特徴量を追加します。","execution_count":null},{"metadata":{"_cell_guid":"db57aabd-0e26-9ff9-9ebd-56d401cdf6e8","_uuid":"c0e1f01b3f58e8f31b938b0e5eb1733132edc8ad","trusted":true},"cell_type":"code","source":"# grid = sns.FacetGrid(train_df, col='Embarked')\ngrid = sns.FacetGrid(train_df, row='Embarked', size=2.2, aspect=1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6b3f73f4-4600-c1ce-34e0-bd7d9eeb074a","_uuid":"fd824f937dcb80edd4117a2927cc0d7f99d934b8"},"cell_type":"markdown","source":"\n\nまた、（数値以外の値を持つ）カテゴリカルな特徴と数値的な特徴を相関させたい場合もあります。Embarked（数値以外のカテゴリ）、Sex（数値以外のカテゴリ）、Fare（数値の連続）、Survived（数値のカテゴリ）との関連付けを検討することができます。\n\n**観察結果。\n\n- 高い運賃を払っている乗客の方が生存率が高い。(#4)運賃の範囲を作成するための我々の仮定を確認しました。\n- 乗船地は生存率と相関しています。(#1)と(#2)の相関を確認しました。\n\n**決定。\n\n- 運賃機能のバンディングを検討。","execution_count":null},{"metadata":{"_cell_guid":"a21f66ac-c30d-f429-cc64-1da5460d16a9","_uuid":"c8fd535ac1bc90127369027c2101dbc939db118e","trusted":true},"cell_type":"code","source":"# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})\ngrid = sns.FacetGrid(train_df, row='Embarked', col='Survived', size=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cfac6291-33cc-506e-e548-6cad9408623d","_uuid":"73a9111a8dc2a6b8b6c78ef628b6cae2a63fc33f"},"cell_type":"markdown","source":"## データを書き換える\n\n私たちは、データセットとソリューションの要件に関するいくつかの仮定と決定を収集しました。これまでのところ、私たちはこれらに到達するために単一の機能や値を変更する必要はありませんでした。それでは、修正、作成、目標の完成のための決定と仮定を実行してみましょう。\n\n### 機能を削除して修正する\n\nこれは実行するための良い出発点です。機能を削除することで、扱うデータポイントを減らすことができます。ノートブックのスピードが上がり、分析が楽になります。\n\n仮定と決定に基づいて、キャビン（修正2）とチケット（修正1）の特徴を削除したいと思います。\n\n必要に応じて、一貫性を保つために、トレーニングデータセットとテストデータセットの両方で操作を実行することに注意してください。","execution_count":null},{"metadata":{"_cell_guid":"da057efe-88f0-bf49-917b-bb2fec418ed9","_uuid":"e328d9882affedcfc4c167aa5bb1ac132547558c","trusted":true},"cell_type":"code","source":"print(\"Before\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)\n\ntrain_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\ntest_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\ncombine = [train_df, test_df]\n\n\"After\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6b3a1216-64b6-7fe2-50bc-e89cc964a41c","_uuid":"21d5c47ee69f8fbef967f6f41d736b5d4eb6596f"},"cell_type":"markdown","source":"### 既存の機能から抽出して新しい機能を作成する\n\nName特徴とPassengerId特徴を削除する前に、Name特徴がタイトルを抽出できるかどうかを分析し、タイトルと生存率の相関関係をテストしたいと考えています。\n\n以下のコードでは、正規表現を用いてタイトル特徴を抽出している。RegExパターン `(\\w+\\..)` は、Name特徴量の中でドット文字で終わる最初の単語にマッチする。フラグ `expand=False` はDataFrameを返す。\n\n**観察してみた\n\nTitle、Age、Survivedをプロットすると、以下の観察結果が得られます。\n\n- ほとんどのタイトルは、年齢グループを正確に分類しています。例えば、以下のようになります。マスタータイトルの平均年齢は5歳です。\n- タイトル間の生存率は、年齢帯で若干の差がある。\n- 特定のタイトルはほとんどが生存している（Mme, Lady, Sir）か、そうでない（Don, Rev, Jonkheer）。\n\n**決定。\n\n- モデル学習のために、新しいタイトル機能を保持することにした。","execution_count":null},{"metadata":{"_cell_guid":"df7f0cd4-992c-4a79-fb19-bf6f0c024d4b","_uuid":"c916644bd151f3dc8fca900f656d415b4c55e2bc","trusted":true},"cell_type":"code","source":"for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_df['Title'], train_df['Sex'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"553f56d7-002a-ee63-21a4-c0efad10cfe9","_uuid":"b8cd938fba61fb4e226c77521b012f4bb8aa01d0","trusted":true},"cell_type":"code","source":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"67444ebc-4d11-bac1-74a6-059133b6e2e8","_uuid":"e805ad52f0514497b67c3726104ba46d361eb92c","trusted":true},"cell_type":"code","source":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9d61dded-5ff0-5018-7580-aecb4ea17506","_uuid":"1da299cf2ffd399fd5b37d74fb40665d16ba5347","trusted":true},"cell_type":"code","source":"train_df = train_df.drop(['Name', 'PassengerId'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.shape, test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2c8e84bb-196d-bd4a-4df9-f5213561b5d3","_uuid":"a1ac66c79b279d94860e66996d3d8dba801a6d9a"},"cell_type":"markdown","source":"### カテゴリ特徴量を変換する\n\nこれで、文字列を含む特徴量を数値に変換できるようになりました。これは、ほとんどのモデルアルゴリズムで必要とされています。これを行うことで、特徴量の完成という目標を達成するのにも役立ちます。\n\nまず、Sex特徴量をGenderという新しい特徴量に変換してみましょう（女性=1、男性=0）。","execution_count":null},{"metadata":{"_cell_guid":"c20c1df2-157c-e5a0-3e24-15a828095c96","_uuid":"840498eaee7baaca228499b0a5652da9d4edaf37","trusted":true},"cell_type":"code","source":"for dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d72cb29e-5034-1597-b459-83a9640d3d3a","_uuid":"6da8bfe6c832f4bd2aa1312bdd6b8b4af48a012e"},"cell_type":"markdown","source":"### 数値連続特徴量を完成させる\n\nここで、欠落値またはヌル値を持つ特徴量の推定と補完を開始します。まず、年齢特徴量についてこれを行います。\n\n数値連続特徴量を完成させるには、以下の3つの方法が考えられます。\n\n1. 簡単な方法は、平均と[標準偏差]の間に乱数を生成する方法です(https://en.wikipedia.org/wiki/Standard_deviation)。\n\n2. 2. より正確に欠損値を推測する方法は、他の相関のある特徴量を使用することです。私たちの場合は、Age、Gender、Pclass の間に相関があることに注意してください。Pclass と Gender の特徴の組み合わせのセット間の Age の [中央値](https://en.wikipedia.org/wiki/Median) を使用して Age の値を推測します。つまり、Pclass=1 と Gender=0、Pclass=1 と Gender=1 の場合の年齢の中央値、などです。\n\n3. 3. 方法1と2を組み合わせます。そこで、中央値に基づいて年齢値を推測するのではなく、PclassとGenderの組み合わせの集合に基づいて、平均と標準偏差の間の乱数を使用します。\n\n方法1と3は、我々のモデルにランダム・ノイズを導入します。複数回の実行から得られる結果は異なるかもしれません。我々は方法2を好むでしょう。","execution_count":null},{"metadata":{"_cell_guid":"c311c43d-6554-3b52-8ef8-533ca08b2f68","_uuid":"345038c8dd1bac9a9bc5e2cfee13fcc1f833eee0","trusted":true},"cell_type":"code","source":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')\ngrid = sns.FacetGrid(train_df, row='Pclass', col='Sex', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9299523c-dcf1-fb00-e52f-e2fb860a3920","_uuid":"24a0971daa4cbc3aa700bae42e68c17ce9f3a6e2","trusted":true},"cell_type":"code","source":"guess_ages = np.zeros((2,3))\nguess_ages","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ec9fed37-16b1-5518-4fa8-0a7f579dbc82","_uuid":"8acd90569767b544f055d573bbbb8f6012853385"},"cell_type":"markdown","source":"ここで、Sex (0 または 1) と Pclass (1, 2, 3) を反復して、6 つの組み合わせの Age の推測値を計算します。","execution_count":null},{"metadata":{"_cell_guid":"a4015dfa-a0ab-65bc-0cbe-efecf1eb2569","_uuid":"31198f0ad0dbbb74290ebe135abffa994b8f58f3","trusted":true},"cell_type":"code","source":"for dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_df.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"725d1c84-6323-9d70-5812-baf9994d3aa1","_uuid":"5c8b4cbb302f439ef0d6278dcfbdafd952675353","trusted":true},"cell_type":"code","source":"train_df['AgeBand'] = pd.cut(train_df['Age'], 5)\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"797b986d-2c45-a9ee-e5b5-088de817c8b2","_uuid":"ee13831345f389db407c178f66c19cc8331445b0","trusted":true},"cell_type":"code","source":"for dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"875e55d4-51b0-5061-b72c-8a23946133a3","_uuid":"1ea01ccc4a24e8951556d97c990aa0136da19721","trusted":true},"cell_type":"code","source":"train_df = train_df.drop(['AgeBand'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1c237b76-d7ac-098f-0156-480a838a64a9","_uuid":"e3d4a2040c053fbd0486c8cfc4fec3224bd3ebb3"},"cell_type":"markdown","source":"### 既存の機能を組み合わせて新しい機能を作成する\n\nParchとSibSpを組み合わせたFamilySizeの新機能を作ることができます。これにより、データセットから Parch と SibSp を削除できるようになります。","execution_count":null},{"metadata":{"_cell_guid":"7e6c04ed-cfaa-3139-4378-574fd095d6ba","_uuid":"33d1236ce4a8ab888b9fac2d5af1c78d174b32c7","trusted":true},"cell_type":"code","source":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5c778c69-a9ae-1b6b-44fe-a0898d07be7a","_uuid":"3b8db81cc3513b088c6bcd9cd1938156fe77992f","trusted":true},"cell_type":"code","source":"for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e6b87c09-e7b2-f098-5b04-4360080d26bc","_uuid":"3da4204b2c78faa54a94bbad78a8aa85fbf90c87"},"cell_type":"markdown","source":"Let us drop Parch, SibSp, and FamilySize features in favor of IsAlone.","execution_count":null},{"metadata":{"_cell_guid":"74ee56a6-7357-f3bc-b605-6c41f8aa6566","_uuid":"1e3479690ef7cd8ee10538d4f39d7117246887f0","trusted":true},"cell_type":"code","source":"train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntest_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ncombine = [train_df, test_df]\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f890b730-b1fe-919e-fb07-352fbd7edd44","_uuid":"71b800ed96407eba05220f76a1288366a22ec887"},"cell_type":"markdown","source":"We can also create an artificial feature combining Pclass and Age.","execution_count":null},{"metadata":{"_cell_guid":"305402aa-1ea1-c245-c367-056eef8fe453","_uuid":"aac2c5340c06210a8b0199e15461e9049fbf2cff","trusted":true},"cell_type":"code","source":"for dataset in combine:\n    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n\ntrain_df.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"13292c1b-020d-d9aa-525c-941331bb996a","_uuid":"8264cc5676db8cd3e0b3e3f078cbaa74fd585a3c"},"cell_type":"markdown","source":"### 範疇の特徴量を完成させる\n\nエンバークされた特徴量は，乗船した港に基づいて S, Q, C の値を取る．我々の訓練データセットには2つの欠損値がある。我々は、これらの欠落値を単に最も一般的な頻度で埋めています。","execution_count":null},{"metadata":{"_cell_guid":"bf351113-9b7f-ef56-7211-e8dd00665b18","_uuid":"1e3f8af166f60a1b3125a6b046eff5fff02d63cf","trusted":true},"cell_type":"code","source":"freq_port = train_df.Embarked.dropna().mode()[0]\nfreq_port","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"51c21fcc-f066-cd80-18c8-3d140be6cbae","_uuid":"d85b5575fb45f25749298641f6a0a38803e1ff22","trusted":true},"cell_type":"code","source":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ntrain_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f6acf7b2-0db3-e583-de50-7e14b495de34","_uuid":"d8830e997995145314328b6218b5606df04499b0"},"cell_type":"markdown","source":"### カテゴライズされた特徴量を数値に変換する\n\n新しい数値Port機能を作成することで、EmbarkedFill機能を変換できるようになりました。","execution_count":null},{"metadata":{"_cell_guid":"89a91d76-2cc0-9bbb-c5c5-3c9ecae33c66","_uuid":"e480a1ef145de0b023821134896391d568a6f4f9","trusted":true},"cell_type":"code","source":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e3dfc817-e1c1-a274-a111-62c1c814cecf","_uuid":"d79834ebc4ab9d48ed404584711475dbf8611b91"},"cell_type":"markdown","source":"### 数値機能を素早く完成させて変換する\n\nこれで、テストデータセット内の単一の欠落値に対する運賃特徴量を、この特徴量に対して最も頻繁に発生する値を取得するためのモードを使用して完成させることができるようになりました。これを1行のコードで行います。\n\n1つの値を置き換えるだけなので、中間的な新しい特徴を作成したり、欠落した特徴を推測するための相関関係のための更なる分析を行ったりしていないことに注意してください。この完成目標は、モデルアルゴリズムが非ヌル値で動作するようにするために必要な要件を達成しています。\n\nまた、運賃は通貨を表しているので、小数点以下2桁に丸めたいかもしれません。","execution_count":null},{"metadata":{"_cell_guid":"3600cb86-cf5f-d87b-1b33-638dc8db1564","_uuid":"aacb62f3526072a84795a178bd59222378bab180","trusted":true},"cell_type":"code","source":"test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4b816bc7-d1fb-c02b-ed1d-ee34b819497d","_uuid":"3466d98e83899d8b38a36ede794c68c5656f48e6"},"cell_type":"markdown","source":"We can not create FareBand.","execution_count":null},{"metadata":{"_cell_guid":"0e9018b1-ced5-9999-8ce1-258a0952cbf2","_uuid":"b9a78f6b4c72520d4ad99d2c89c84c591216098d","trusted":true},"cell_type":"code","source":"train_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d65901a5-3684-6869-e904-5f1a7cce8a6d","_uuid":"89400fba71af02d09ff07adf399fb36ac4913db6"},"cell_type":"markdown","source":"Convert the Fare feature to ordinal values based on the FareBand.","execution_count":null},{"metadata":{"_cell_guid":"385f217a-4e00-76dc-1570-1de4eec0c29c","_uuid":"640f305061ec4221a45ba250f8d54bb391035a57","trusted":true},"cell_type":"code","source":"for dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain_df = train_df.drop(['FareBand'], axis=1)\ncombine = [train_df, test_df]\n    \ntrain_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"27272bb9-3c64-4f9a-4a3b-54f02e1c8289","_uuid":"531994ed95a3002d1759ceb74d9396db706a41e2"},"cell_type":"markdown","source":"And the test dataset.","execution_count":null},{"metadata":{"_cell_guid":"d2334d33-4fe5-964d-beac-6aa620066e15","_uuid":"8453cecad81fcc44de3f4e4e4c3ce6afa977740d","trusted":true},"cell_type":"code","source":"test_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"69783c08-c8cc-a6ca-2a9a-5e75581c6d31","_uuid":"a55f20dd6654610ff2d66c1bf3e4c6c73dcef9e5"},"cell_type":"markdown","source":"## ♪ モデル、予測、解決\n\nこれで、モデルを訓練し、必要な解を予測する準備が整いました。60以上の予測モデリングアルゴリズムがあります。問題のタイプと解決の要件を理解して、評価できる少数のモデルに絞り込む必要があります。私たちの問題は、分類と回帰の問題です。我々は、出力（生存しているかどうか）と他の変数や特徴（性別、年齢、港...）との間の関係を識別したいと考えています。我々はまた、与えられたデータセットでモデルを訓練しているので、教師付き学習と呼ばれる機械学習のカテゴリを完成させています。これら2つの基準-教師付き学習に分類と回帰を加えれば、モデルの選択をいくつかに絞ることができます。これらには次のようなものがあります。\n\n- ロジスティック回帰\n- KNNまたはk-Nearest Neighbors\n- サポートベクターマシン\n- ナイーブベイズ分類器\n- 決定木\n- ランダムフォレスト\n- パーセプトロン\n- 人工ニューラルネットワーク\n- RVMまたは関連性ベクトルマシン","execution_count":null},{"metadata":{"_cell_guid":"0acf54f9-6cf5-24b5-72d9-29b30052823a","_uuid":"04d2235855f40cffd81f76b977a500fceaae87ad","trusted":true},"cell_type":"code","source":"X_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"579bc004-926a-bcfe-e9bb-c8df83356876","_uuid":"782903c09ec9ee4b6f3e03f7c8b5a62c00461deb"},"cell_type":"markdown","source":"ロジスティック回帰は、ワークフローの初期に実行すると便利なモデルです。ロジスティック回帰は、累積ロジスティック分布であるロジスティック関数を用いて確率を推定することで、カテゴリカルな従属変数（特徴量）と1つ以上の独立変数（特徴量）との関係を測定します。参考文献[Wikipedia](https://en.wikipedia.org/wiki/Logistic_regression)。\n\n我々の訓練データセットに基づいてモデルが生成した信頼度スコアに注目してください。","execution_count":null},{"metadata":{"_cell_guid":"0edd9322-db0b-9c37-172d-a3a4f8dec229","_uuid":"a649b9c53f4c7b40694f60f5c8dc14ec5ef519ec","trusted":true},"cell_type":"code","source":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3af439ae-1f04-9236-cdc2-ec8170a0d4ee","_uuid":"180e27c96c821656a84889f73986c6ddfff51ed3"},"cell_type":"markdown","source":"ロジスティック回帰を使用して、特徴の作成と目標の完了のための仮定と決定を検証することができます。これは、決定関数の特徴の係数を計算することで行うことができます。\n\n正の係数は、応答の対数オッズを増加させ(したがって、確率を増加させ)、負の係数は、応答の対数オッズを減少させ(したがって、確率を減少させ)ます。\n\n- Sexが最も高い正の係数で、Sexの値が増加すると（男性：0から女性：1）、Survived=1の確率が最も高くなることを示唆しています。\n- 逆にPclassが高くなると、生存している確率は最も低くなります。\n- このように、Age*ClassはSurvivedと2番目に高い負の相関があるので、モデル化するのに適した人工的な特徴です。\n- また、タイトルも2番目に高い正の相関があります。","execution_count":null},{"metadata":{"_cell_guid":"e545d5aa-4767-7a41-5799-a4c5e529ce72","_uuid":"6e6f58053fae405fc93d312fc999f3904e708dbe","trusted":true},"cell_type":"code","source":"coeff_df = pd.DataFrame(train_df.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ac041064-1693-8584-156b-66674117e4d0","_uuid":"ccba9ac0a9c3c648ef9bc778977ab99066ab3945"},"cell_type":"markdown","source":"次に、サポートベクターマシンを使用してモデル化します。これは、分類と回帰分析に使用されるデータを分析する学習アルゴリズムを持つ教師付き学習モデルです。2つのカテゴリ**のうちの1つまたは他のカテゴリ**に属するとマークされた訓練サンプルのセットが与えられると、SVM訓練アルゴリズムは、新しいテストサンプルを1つのカテゴリまたは他のカテゴリに割り当てるモデルを構築し、それを非確率的なバイナリ線形分類器にします。参考文献[Wikipedia](https://en.wikipedia.org/wiki/Support_vector_machine)。\n\nこのモデルは、ロジスティクス回帰モデルよりも高い信頼度スコアを生成することに注意してください。","execution_count":null},{"metadata":{"_cell_guid":"7a63bf04-a410-9c81-5310-bdef7963298f","_uuid":"60039d5377da49f1aa9ac4a924331328bd69add1","trusted":true},"cell_type":"code","source":"# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"172a6286-d495-5ac4-1a9c-5b77b74ca6d2","_uuid":"bb3ed027c45664148b61e3aa5e2ca8111aac8793"},"cell_type":"markdown","source":"パターン認識では、k-最近傍アルゴリズム（略してk-NN）は、分類と回帰に使用されるノンパラメトリック手法です。サンプルは、隣人の過半数の投票によって分類され、サンプルは、そのk個の最も近い隣人（kは正の整数で、通常は小さい）の中で最も一般的なクラスに割り当てられます。k = 1の場合、オブジェクトは単にその1つの最も近い隣人のクラスに割り当てられます。参考文献[Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)。\n\nKNNの信頼度スコアは、ロジスティック回帰よりは良いが、SVMよりは悪い。","execution_count":null},{"metadata":{"_cell_guid":"ca14ae53-f05e-eb73-201c-064d7c3ed610","_uuid":"54d86cd45703d459d452f89572771deaa8877999","trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"810f723d-2313-8dfd-e3e2-26673b9caa90","_uuid":"1535f18113f851e480cd53e0c612dc05835690f3"},"cell_type":"markdown","source":"機械学習では、ナイーブベイズ分類器は、特徴間の強い（ナイーブな）独立性を仮定したベイズの定理の適用に基づく単純な確率的分類器のファミリである。ナイーブベイズ分類器は、学習問題の変数（特徴量）の数に比例した数のパラメータを線形に必要とする、拡張性の高い分類器である。参考文献[Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)。\n\nモデル生成信頼度スコアは、これまでに評価されたモデルの中で最も低い。","execution_count":null},{"metadata":{"_cell_guid":"50378071-7043-ed8d-a782-70c947520dae","_uuid":"723c835c29e8727bc9bad4b564731f2ca98025d0","trusted":true},"cell_type":"code","source":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1e286e19-b714-385a-fcfa-8cf5ec19956a","_uuid":"df148bf93e11c9ec2c97162d5c0c0605b75d9334"},"cell_type":"markdown","source":"パーセプトロンは，2値分類器（数値のベクトルで表される入力が，ある特定のクラスに属するか否かを決定できる関数）の教師付き学習のためのアルゴリズムである．これは線形分類器の一種であり，特徴ベクトルと重みを組み合わせた線形予測関数に基づいて予測を行う分類アルゴリズムである．このアルゴリズムは、訓練セットの要素を一度に一つずつ処理するという意味で、オンライン学習を可能にしている。参考文献[ウィキペディア](https://en.wikipedia.org/wiki/Perceptron)。","execution_count":null},{"metadata":{"_cell_guid":"ccc22a86-b7cb-c2dd-74bd-53b218d6ed0d","_uuid":"c19d08949f9c3a26931e28adedc848b4deaa8ab6","trusted":true},"cell_type":"code","source":"# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a4d56857-9432-55bb-14c0-52ebeb64d198","_uuid":"52ea4f44dd626448dd2199cb284b592670b1394b","trusted":true},"cell_type":"code","source":"# Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dc98ed72-3aeb-861f-804d-b6e3d178bf4b","_uuid":"3a016c1f24da59c85648204302d61ea15920e740","trusted":true},"cell_type":"code","source":"# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bae7f8d7-9da0-f4fd-bdb1-d97e719a18d7","_uuid":"1c70e99920ae34adce03aaef38d61e2b83ff6a9c"},"cell_type":"markdown","source":"このモデルは、特徴（木の枝）を目標値（木の葉）に関する結論にマッピングする予測モデルとして決定木を使用します。これらの木構造では、葉はクラス・ラベルを表し、枝はそれらのクラス・ラベルにつながる特徴の接続を表します。対象変数が連続的な値（典型的には実数）を取ることができる決定木は、回帰木と呼ばれる。参考文献[Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning)。\n\nモデル信頼度スコアは、これまでに評価されたモデルの中で最も高い。","execution_count":null},{"metadata":{"_cell_guid":"dd85f2b7-ace2-0306-b4ec-79c68cd3fea0","_uuid":"1f94308b23b934123c03067e84027b507b989e52","trusted":true},"cell_type":"code","source":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"85693668-0cd5-4319-7768-eddb62d2b7d0","_uuid":"24f4e46f202a858076be91752170cad52aa9aefa"},"cell_type":"markdown","source":"次のモデルであるランダムフォレストは、最もポピュラーなモデルの一つです。ランダムフォレストまたはランダムデシジョンフォレストとは、分類や回帰などのためのアンサンブル学習法であり、学習時に多数の決定木（n_estimators=100）を構築し、個々の木のクラス（分類）または平均予測（回帰）のモードとなるクラスを出力することで動作する。参考文献[Wikipedia](https://en.wikipedia.org/wiki/Random_forest)。\n\nモデルの信頼度スコアは、これまでに評価されたモデルの中で最も高い。このモデルの出力(Y_pred)を結果のコンペ提出物の作成に利用することにしました。","execution_count":null},{"metadata":{"_cell_guid":"f0694a8e-b618-8ed9-6f0d-8c6fba2c4567","_uuid":"483c647d2759a2703d20785a44f51b6dee47d0db","trusted":true},"cell_type":"code","source":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f6c9eef8-83dd-581c-2d8e-ce932fe3a44d","_uuid":"2c1428d022430ea594af983a433757e11b47c50c"},"cell_type":"markdown","source":"### モデルの評価\n\nこれで、すべてのモデルの評価をランク付けして、問題に最も適したモデルを選択することができます。決定木とランダムフォレストのスコアは同じですが、決定木が学習セットにオーバーフィットする癖を修正するため、ランダムフォレストを使用することにしました。","execution_count":null},{"metadata":{"_cell_guid":"1f3cebe0-31af-70b2-1ce4-0fd406bcdfc6","_uuid":"06a52babe50e0dd837b553c78fc73872168e1c7d","trusted":true},"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"28854d36-051f-3ef0-5535-fa5ba6a9bef7","_uuid":"82b31ea933b3026bd038a8370d651efdcdb3e4d7","trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv(\"submit.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"aeec9210-f9d8-cd7c-c4cf-a87376d5f693","_uuid":"cdae56d6adbfb15ff9c491c645ae46e2c91d75ce"},"cell_type":"markdown","source":"## References\n\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\n\n- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)\n- [Getting Started with Pandas: Kaggle's Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)\n- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
